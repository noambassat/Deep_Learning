{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "ex5.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT__egtUhUo1"
      },
      "source": [
        "# Deep Learning: Ex.5 - CIFAR-10 take 2\n",
        "\n",
        "Submitted by: Noam Bassat 308465434\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EghsB7HxhUo4"
      },
      "source": [
        "# TensorFlow and tf.keras\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten,Conv2D,MaxPooling2D,Dropout,BatchNormalization\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from seaborn import heatmap \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNpWxAbrhUo5"
      },
      "source": [
        "### Load the CIFAR-10 Dataset\n",
        "\n",
        "When running this command for the first time, it will download dataset from a remote server, which might take some time.. (in case of server error - just try again a bit later)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktt8T5k_hUo5"
      },
      "source": [
        "# 1. load/download the data\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# 2. flatten the labels (easier to deal with)\n",
        "train_labels = train_labels.flatten()  # (50000, 1) -> (50000,)\n",
        "test_labels = test_labels.flatten()    # (10000, 1) -> (10000,)\n",
        "\n",
        "# 3. convert uint8->float32 and normalize range to 0.0-1.0 \n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "\n",
        "# 4. define the 10 classes names\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# 5. print the shapes\n",
        "print('train_images.shape =',train_images.shape)\n",
        "print('train_labels.shape =',train_labels.shape)\n",
        "print('test_images.shape =',test_images.shape)\n",
        "print('test_labels.shape =',test_labels.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMFl8VkkhUo6"
      },
      "source": [
        "***\n",
        "\n",
        "- We will try different models (build, fit on training data, and evaluate on test data).\n",
        "\n",
        "- The input layer is the images (32x32x3), and the output layer is a `softmax` 10 units (one unit for each class).\n",
        "\n",
        "- For each model, plot the train/test loss & accuracy plots (as shown in class).\n",
        "\n",
        "- Summarize the results of all models in the table below:\n",
        "\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Model</th>\n",
        "    <th>#parameters</th>\n",
        "    <th>epochs</th>\n",
        "    <th>train accuracy</th>\n",
        "    <th>test accuracy</th>\n",
        "  </tr>\n",
        "    \n",
        "  <!-- copy this block once for every model you tested -->  \n",
        "  <tr> \n",
        "    <td>1. VGG-like model</td>   <!-- Model -->\n",
        "    <td>1,341,226 </td> <!-- #parameters -->\n",
        "    <td>60</td> <!-- epochs -->\n",
        "    <td> 0.9756</td> <!-- train accuracy -->\n",
        "    <td> 0.7789</td> <!-- test accuracy -->\n",
        "  </tr>\n",
        "    \n",
        "   <tr> \n",
        "    <td>2. Batch Normalization</td>   <!-- Model -->\n",
        "    <td>1,345,066 </td> <!-- #parameters -->\n",
        "    <td>60</td> <!-- epochs -->\n",
        "    <td>0.9945</td> <!-- train accuracy -->\n",
        "    <td>0.7868</td> <!-- test accuracy -->\n",
        "  </tr>\n",
        "\n",
        "  <tr> \n",
        "    <td>3. Data Augmentation</td>   <!-- Model -->\n",
        "    <td>1,345,066</td> <!-- #parameters -->\n",
        "    <td>60</td> <!-- epochs -->\n",
        "    <td>0.6704 </td> <!-- train accuracy -->\n",
        "    <td>0.7404</td> <!-- test accuracy -->\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEVjHizyhUo6"
      },
      "source": [
        " \n",
        "def compile_and_train_the_model(model, epochs_num):\n",
        "    # compile the model:\n",
        "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # train the model - with validation\n",
        "    history = model.fit(train_images, train_labels, epochs=epochs_num, batch_size=128, \n",
        "                        validation_data=(test_images,test_labels))\n",
        "    \n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfMO7KZshUo7"
      },
      "source": [
        "***\n",
        "### 1. VGG-like model\n",
        "\n",
        "Apply the following layers (between the input and output layers):\n",
        "- **Block1:** 32-`Conv2D` + 32-`Conv2D` +  `MaxPooling` \n",
        "- **Block2:** 64-`Conv2D` + 64-`Conv2D` +  `MaxPooling` \n",
        "- **Block3:** 128-`Conv2D` + 128-`Conv2D` +  `MaxPooling` \n",
        "- **Block4:** 512-`Dense` + `Dropout(0.5)` \n",
        "\n",
        "Use 3x3 kernerls and `padding='same'` in all Conv2D layers.\n",
        "\n",
        "Use SGD+Momentum or Adam optimizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEL7AZKBhUo7"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32,kernel_size = (3,3), activation='relu',padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(Conv2D(32,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(Conv2D(64,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(128,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(Conv2D(128,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6pYYfgchUo8"
      },
      "source": [
        "history = compile_and_train_the_model(model,60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTLtZ607hUo9"
      },
      "source": [
        "***\n",
        "### 2. Batch Normalization\n",
        "\n",
        "Use the previous model (VGG-like), and add a `BatchNormalization` layer after each of the `Conv2D` or `Dense` layers (except the output layer of course).\n",
        "\n",
        "Use the same running options as before (batch size, epochs, optimizer), but **use a different variable** to record the `history` of the training results (don't run over the previous one..).\n",
        "\n",
        "You should expect the same level of accuracy, but at a shorter convergence time (=less epochs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri7QrBtVhUo9"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32,kernel_size = (3,3), activation='relu',padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(128,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e24D7ZthUo-"
      },
      "source": [
        "history2 = compile_and_train_the_model(model,60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g87S5QJahUo-"
      },
      "source": [
        "***\n",
        "### 3. Data Augmentation\n",
        "\n",
        "In order to acheive better results (higher validation accuracy), we will try to use data augmentation.\n",
        "\n",
        "Use Keras's `ImageDataGenerator` as described in class to re-train your last model (VGG-like + BN). \n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
        "\n",
        "Note that each batch is taking more time to train, as the we need to re-generate each batch of sample.\n",
        "\n",
        "Again, **use a different variable** to record the history of the training results.\n",
        "\n",
        "Note, when using data augmentation inside `model.fit()`, you must specify `steps_per_epoch` (as the batch is generated outside of this method, and its size is therefore unknown). This number should be set as the (integer) number of total training samples (50,000) divided by the batch size (defined inside the generator `.flow` method).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Ozk8tIhUo_"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.1,)\n",
        "\n",
        "data_iter = datagen.flow(train_images, train_labels, batch_size=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5fgA6zNhUo_"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32,kernel_size = (3,3), activation='relu',padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(128,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128,kernel_size = (3,3), activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d14ADTlbhUpA"
      },
      "source": [
        "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSKFsam4mgDS"
      },
      "source": [
        "history3 = model.fit(data_iter, steps_per_epoch=train_images.shape[0]//10, epochs=60, \n",
        "                    validation_data=(test_images,test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GqY_f25hUpA"
      },
      "source": [
        "---\n",
        "### Graphical comparison \n",
        "\n",
        "For each of the benchmark graphs (train-loss, val-loss, train-acc, val-loss) plot a single graph with all 3 runs (use different color for each of the runs).\n",
        "\n",
        "Add graph labels and legends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHAVlytkm9JM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qYPc5vQhUpB"
      },
      "source": [
        "plt.figure(figsize=(14,4))\n",
        "plt.subplot(1,4,1)\n",
        "plt.plot(history.history['loss'],'r',label = \"1\")\n",
        "plt.plot(history2.history['loss'],'b',label = \"2\")\n",
        "plt.plot(history3.history['loss'],'g',label = \"3\")\n",
        "plt.title('Loss',fontsize=14)\n",
        "plt.xlabel('Epochs',fontsize=14)\n",
        "plt.legend(('1','2','3'))\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,4,2)\n",
        "plt.plot(history.history['accuracy'],'r',label = \"1\")\n",
        "plt.plot(history2.history['accuracy'],'b',label = \"2\")\n",
        "plt.plot(history3.history['accuracy'],'g',label = \"3\")\n",
        "plt.ylim([0, 1])\n",
        "plt.title('Accuracy',fontsize=12)\n",
        "plt.xlabel('Epochs',fontsize=12)\n",
        "plt.legend(('1','2','3'))\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,4,3)\n",
        "plt.plot(history.history['val_loss'],'r',label = \"1\")\n",
        "plt.plot(history2.history['val_loss'],'b',label = \"2\")\n",
        "plt.plot(history3.history['val_loss'],'g',label = \"3\")\n",
        "plt.ylim([0, 1])\n",
        "plt.title('val_loss',fontsize=12)\n",
        "plt.xlabel('Epochs',fontsize=12)\n",
        "plt.legend(('1','2','3'))\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,4,4)\n",
        "plt.plot(history.history['val_accuracy'],'r',label = \"1\")\n",
        "plt.plot(history2.history['val_accuracy'],'b',label = \"2\")\n",
        "plt.plot(history3.history['val_accuracy'],'g',label = \"3\")\n",
        "plt.ylim([0, 1])\n",
        "plt.title('val_accuracy',fontsize=12)\n",
        "plt.legend(('1','2','3'))\n",
        "plt.xlabel('Epochs',fontsize=12)\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbydvNDThUpB"
      },
      "source": [
        "***\n",
        "## Good Luck!"
      ]
    }
  ]
}